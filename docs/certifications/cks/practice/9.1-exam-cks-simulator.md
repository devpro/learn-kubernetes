# CKS Simulator Kubernetes 1.28

## Initial setup

Set alias:

```bash
alias k=kubectl
```

Open [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/).

## Question 1

Write all kubeconfig context names into a single file, one per line.

Extract the certificate of one kubeconfig user and write it decoded to a new file.

<details>
  <summary>Solution 1</summary>

From the default shell:

```bash
# writes all kubeconfig context names in a file (two ways to do it)
k config get-contexts -o name > kubecontexts.txt
k config view -o jsonpath='{.contexts[*].name}' | tr ' ' '\n' > kubecontexts.txt

# extracts the certificate and decodes it to a file
k config view --raw -o jsonpath='{.users[?(@.name == "myuser")].user.client-certificate-data}' | base64 --decode > usercert.txt

```

</details>

## Question 2

Falco is installed with default configuration on the first cluster node. Use it to:

* Find a _Pod_ running image `nginx` which creates unwanted package management processes inside its container.

* Find a _Pod_ running image `httpd` which modifies `/etc/passwd`.

Save the Falco logs for case 1 under `falco_case1.log` in format:

```txt
time-with-nanosconds,container-id,container-name,user-name
```

No other information should be in any line. Collect the logs for at least 30 seconds.

Afterwards remove the threats (both 1 and 2) by scaling the replicas of the _Deployments_ that control the offending _Pods_ down to 0.

<details>
  <summary>Solution 2</summary>

From the cluster node running Falco:

```bash
# checks Falco is correctly running as a service
sudo service falco status

# displays Falco configuration, in particular the output (going to syslog)
more /etc/falco/falco.yaml

# prints latest logs
cat /var/log/syslog | grep falco | grep nginx
cat /var/log/syslog | grep falco | grep httpd

# retrieves pod name
crictl ps -id <container_id>
crictl pods -id <pod_id>

# stops Falso service to be able to run it manually with new configuration
sudo service falco stop

# get the keys of the fields to be displayed
falco --list

# finds the rule with a vi search and copy the rule
vi /etc/falco/falco_rules.yaml

# creates local rule file with the copied and updated rule (%evt.time,%container.id,%container.name,%user.name)
vi falco_rules.local.yaml

# runs Falco on this rule for 30 seconds
falco -r falco_rules.local.yaml -M 30 > falco_case1.log

# edit the deployments to update the replicas to 0
k -n team-blue scale deploy webapi --replicas 0
k -n team-purple scale deploy rating-service --replicas 0
```

</details>

## Question 3

You received a list from the DevSecOps team which performed a security investigation of the Kubernetes cluster. The list states the following about the apiserver setup:

* Accessible through a NodePort Service

Change the apiserver setup so that:

* Only accessible through a ClusterIP Service

<details>
  <summary>Solution 3</summary>

From the control plane node:

```bash
# updates Kubernetes API Server configuration
vi /etc/kubernetes/manifests/kube-apiserver.yaml
# - --kubernetes-service-node-port 31000
# + --kubernetes-service-node-port 0

# deletes the service so it is created again
k delete svc kubernetes

# checks the service has type ClusterIP
k get svc
```

</details>

## Question 4

There is _Deployment_ `container-host-hacker` in _Namespace_ `team-red` which mounts `/run/containerd` as a hostPath volume on the _Node_ where it's running. This means that the _Pod_ can access various data about other containers running on the same _Node_.

To prevent this, configure _Namespace_ `team-red` to enforce the `baseline` Pod Security Standard. Once completed, delete the _Pod_ of the Deployment mentioned above.

Check the _ReplicaSet_ events and write the event/log lines containing the reason why the _Pod_ isn't recreated into `ex4-replicaset.log`.

<details>
  <summary>Solution 4</summary>

From the default shell:

```bash
# edits the namespace to add the label
k edit ns team-red

# displays the new version
cat <<EOF
apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: team-red
    pod-security.kubernetes.io/enforce: baseline
  name: team-red
EOF

# deletes the pod
k delete pod container-host-hacker-xxx -n team-red

# get events
k events -n team-red > ex4-replicaset.log
```

</details>

## Question 5

You're asked to evaluate specific settings of a Kubernetes cluster against the CIS Benchmark recommendations. Use the tool `kube-bench` which is already installed on the nodes.

On the control plane node ensure (correct if necessary) that the CIS recommendations are met for:

* The `--profiling` argument of the `kube-controller-manager`

* The ownership of directory `/var/lib/etcd`

On the worker node ensure (correct if necessary) that the CIS recommendations are met for:

* The permissions of the kubelet configuration `/var/lib/kubelet/config.yaml`

* The `--client-ca-file` argument of the `kubelet`

<details>
  <summary>Solution 5</summary>

From the control plane node:

```bash
# runs kube-bench to identify the related remedations
kube-bench run --targets=master

# edit manifest file to add "--profiling" in kube-controller-manager argument
vi /etc/kubernetes/manifests/kube-controller-manager.yaml

# fixes folder ownership
chown -R etcd:etcd /var/lib/etcd
```

From the worker node:

```bash
# runs kube-bench to identify the related remedations
kube-bench run --targets=node

# fixes folder permission
chmod 644 /var/lib/kubelet/config.yaml

# looks at kubelet options
ps -ef | grep kubelet

# edit manifest file to set clientCAFile
sudo vi /var/lib/kubelet/config.yaml

# if a change was done, restarts the service
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

</details>

## Question 6

There are four Kubernetes server binaries located at `binaries` folder.

```txt
kube-apiserver f417c0555bc0167355589dd1afe23be9bf909bf98312b1025f12015d1b58a1c62c9908c0067a7764fa35efdac7016a9efa8711a44425dd6692906a7c283f032c
kube-controller-manager 60100cc725e91fe1a949e1b2d0474237844b5862556e25c2c655a33boa8225855ec5ee22fa4927e6c46a60d43a7c4403a27268f96fbb726307d1608b44f38a60
kube-proxy 52f9d8ad045f8eee1d689619ef8ceef2d86d50c75a6a332653240d7ba5b2a114aca056d9e513984ade24358c9662714973c1960c62a5cb37dd375631c8a614c6
kubelet 4be40f2440619e990897cf956c32800dc96c2c983bf64519854a3309fa5aa21827991559f9c44595098e27e6f2ee4d64a3fdec6baba8a177881f20e3ec61e26c
```

Delete the binaries that don't match with the sha512 values.

<details>
  <summary>Solution 6</summary>

From the default shell:

```bash
echo "f417c0555bc0167355589dd1afe23be9bf909bf98312b1025f12015d1b58a1c62c9908c0067a7764fa35efdac7016a9efa8711a44425dd6692906a7c283f032c  kube-apiserver" | sha512sum --check
echo "60100cc725e91fe1a949e1b2d0474237844b5862556e25c2c655a33boa8225855ec5ee22fa4927e6c46a60d43a7c4403a27268f96fbb726307d1608b44f38a60  kube-controller-manager" | sha512sum --check
echo "52f9d8ad045f8eee1d689619ef8ceef2d86d50c75a6a332653240d7ba5b2a114aca056d9e513984ade24358c9662714973c1960c62a5cb37dd375631c8a614c6  kube-proxy" | sha512sum --check
echo "4be40f2440619e990897cf956c32800dc96c2c983bf64519854a3309fa5aa21827991559f9c44595098e27e6f2ee4d64a3fdec6baba8a177881f20e3ec61e26c  kubelet" | sha512sum --check

rm kube-controller-manager kubelet
```

</details>

## Question 7

The Open Policy Agent and Gatekeeper have been installed to, among other things, enforce blacklisting of certain image registries. Alter the existing constraint and/or template to also blacklist images from `very-bad-registry.com`.

Test it by creating a single _Pod_ using image `very-bad-registry.com/image` in _Namespace_ `default`, it shouldn't work.

You can also verify your changes by looking at the existing Deployment `untrusted` in _Namespace_ `default`, it uses an image from the new untrusted source. The OPA contraint should throw violation messages for this one.

<details>
  <summary>Solution 7</summary>

From the default shell:

```bash
# lists existing contraint templates and constraints
k get constrainttemplate
k get constraint

# edits constrainttemplate to add the bad registry
k edit constrainttemplates blacklistimages

# tries to create a pod on this bad registry
kubectl run test --image=very-bad-registry.com/image -n default

# looks at the constraint output
k describe blacklistimages pod-trusted-images
```

</details>

## Question 8

The Kubernetes Dashboard is installed in Namespace kubernetes-dashboard and is configured to:

* Allow users to "skip login"
* Allow insecure access (HTTP without authentication)
* Allow basic authentication
* Allow access from outside the cluster

You are asked to make it more secure by:

* Deny users to "skip login"
* Deny insecure access, enforce HTTPS (self signed certificates are ok for now)
* Add the --auto-generate-certificates argument
* Enforce authentication using a token (with possibility to use RBAC)
* Allow only cluster internal access

<details>
  <summary>Solution 8</summary>

From the default shell:

```bash
# edits the deployment
k edit deploy kubernetes-dashboard -n kubernetes-dashboard
# template:
#   spec:
#     containers:
#     - args:
#       - --namespace=kubernetes-dashboard  
# +     - --authentication-mode=token
# +     - --auto-generate-certificates
# +     - --enable-skip-login=false
# -     - --enable-skip-login=true
# -     - --enable-insecure-login
#       image: kubernetesui/dashboard:v2.0.3
#       imagePullPolicy: Always
#       name: kubernetes-dashboard

# edits the service to delete nodePorts in ports, update type to ClusterIP and delete externalTrafficPolicy
k -n kubernetes-dashboard edit svc kubernetes-dashboard
```

</details>

## Question 9

Some containers need to run more secure and restricted. There is an existing AppArmor profile located at `ex9-profile` for this.

* Install the AppArmor profile on the first cluster _Node_ `cluster1-node1`
* Add label `security=apparmor` to the _Node_
* Create a _Deployment_ named `apparmor` in _Namespace_ `default` with:
  * One replica of image `nginx:1.19.2`
  * NodeSelector for `security=apparmor`
  * Single container named `c1` with the AppArmor profile enabled

The _Pod_ might not run properly with the profile enabled. Write the logs of the _Pod_ into `ex9-pod.logs` so another team can work on getting the application running.

<details>
  <summary>Solution 9</summary>

From the first cluster node:

```bash
# loads the AppArmor profile
apparmor_parser ./profile

# checks loaded profiles to make sure very-secure is present
apparmor_status

# adds a label on the node
k label node cluster1-node1 security=apparmor

# checks node labels
k get node --show-labels

# creates the deployment manifest
k create deploy apparmor --image=nginx:1.19.2 --dry-run=client -o yaml > 9_deploy.yaml

# updates the manifest
cat > 9_deploy.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apparmor
  name: apparmor
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apparmor
  template:
    metadata:
      labels:
        app: apparmor
      annotations:
        container.apparmor.security.beta.kubernetes.io/c1: localhost/very-secure
    spec:
      nodeSelector:
        security: apparmor
      containers:
        - image: nginx:1.19.2
          name: c1
EOF

# creates the deployment
k apply -f 9_deploy.yaml

# captures the logs in a file
k logs apparmor-xxxx > /opt/course/9/logs
```

</details>

## Question 10

Team purple wants to run some of their workloads more secure. Worker node `cluster1-node2` has container engine containerd already installed and it's configured to support the runsc/gvisor runtime.

Create a _RuntimeClass_ named `gvisor` with handler `runsc`.

Create a _Pod_ that uses the _RuntimeClass_. The _Pod_ should be in _Namespace_ `team-purple`, named `gvisor-test` and of image `nginx:1.19.2`. Make sure the _Pod_ runs on `cluster1-node2`.

Write the `dmesg` output of the successfully started _Pod_ into `/opt/course/10/gvisor-test-dmesg`.

<details>
  <summary>Solution 10</summary>

From the default shell:

```bash
# creates the RuntimeClass
kubectl apply -f - <<EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc
EOF

# creates pod manifest file
k run gvisor-test --image=nginx:1.19.2 -n team-purple --dry-run=client -o yaml > 10_pod.yaml

# updates the manifest
cat > 10_pod.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: gvisor-test
  name: gvisor-test
  namespace: team-purple
spec:
  nodeName: cluster1-node2
  runtimeClassName: gvisor
  containers:
    - image: nginx:1.19.2
      name: gvisor-test
  dnsPolicy: ClusterFirst
  restartPolicy: Always
EOF

# applies the manifest
k apply -f 10_pod.yaml

# captures dmesg output
k exec gvisor-test -n team-purple -- dmesg > /opt/course/10/gvisor-test-dmesg
```

</details>

## Question 11

There is an existing Secret called `database-access` in Namespace `team-green`.

Read the complete Secret content directly from ETCD (using `etcdctl`) and store it into `/opt/course/11/etcd-secret-content`. Write the plain and decoded Secret's value of key "pass" into `/opt/course/11/database-password`.

<details>
  <summary>Solution 11</summary>

From the first cluster node:

```bash
# captures secret value
ETCDCTL_API=3 etcdctl \
--cacert=/etc/kubernetes/pki/etcd/ca.crt   \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key  \
get /registry/secrets/team-green/database-access > /opt/course/11/etcd-secret-content

# saves password
echo xxxx | base64 -d > /opt/course/11/database-password
```

</details>

## Question 12

You're asked to investigate a possible permission escape in Namespace `restricted`. The context authenticates as user `restricted` which has only limited permissions and shouldn't be able to read Secret values.

Try to find the password-key values of the Secrets `secret1`, `secret2` and `secret3` in Namespace `restricted`. Write the decoded plaintext values into files `/opt/course/12/secret1`, `/opt/course/12/secret2` and `/opt/course/12/secret3`.

<details>
  <summary>Solution 12</summary>

TODO

</details>

## Question 13

There is a metadata service available at http://192.168.100.21:32000 on which Nodes can reach sensitive data, like cloud credentials for initialisation. By default, all _Pods_ in the cluster also have access to this endpoint. The DevSecOps team has asked you to restrict access to this metadata server.

In Namespace metadata-access:

* Create a NetworkPolicy named `metadata-deny` which prevents egress to `192.168.100.21` for all _Pods_ but still allows access to everything else
* Create a NetworkPolicy named `metadata-allow` which allows _Pods_ having label `role: metadata-accessor` to access endpoint `192.168.100.21`

There are existing _Pods_ in the target Namespace with which you can test your policies, but don't change their labels.

<details>
  <summary>Solution 13</summary>

From the default shell:

```bash
# creates the deny network policy
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: metadata-deny
  namespace: metadata-access
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 192.168.100.21/32
EOF

# creates the approve network policy
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: metadata-allow
  namespace: metadata-access
spec:
  podSelector:
    matchLabels:
      role: metadata-accessor
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 192.168.100.21/32
EOF
```

</details>

## Question 14

There are _Pods_ in Namespace `team-yellow`. A security investigation noticed that some processes running in these _Pods_ are using the Syscall `kill`, which is forbidden by a Team Yellow internal policy.

Find the offending Pod(s) and remove these by reducing the replicas of the parent Deployment to 0.

<details>
  <summary>Solution 14</summary>

TODO

</details>

## Question 15

In Namespace `team-pink` there is an existing Nginx Ingress resources named `secure` which accepts two paths /app and /api which point to different ClusterIP Services.

From your main terminal you can connect to it using for example:

* HTTP: `curl -v http://secure-ingress.test:31080/app`

* HTTPS: `curl -kv https://secure-ingress.test:31443/app`

Right now it uses a default generated TLS certificate by the Nginx Ingress Controller.

You're asked to instead use the key and certificate provided at `/opt/course/15/tls.key` and `/opt/course/15/tls.crt`. As it's a self-signed certificate you need to use `curl -k` when connecting to it.

<details>
  <summary>Solution 15</summary>

From the default shell:

```bash
# creates the secret for the certificate
k create secret tls tls-secret -n team-pink --key tls.key --cert tls.crt

# edits the deployment to add the tls section
k edit ingress secure -n team-pink
# spec:
# +  tls:
# +    - hosts:
# +        - secure-ingress.test
# +      secretName: tls-secret
```

</details>

## Question 16

There is a Deployment `image-verify` in Namespace `team-blue` which runs image `registry.killer.sh:5000/image-verify:v1`. DevSecOps has asked you to improve this image by:

* Changing the base image to alpine:3.12
* Not installing curl
* Updating nginx to use the version constraint >=1.18.0
* Running the main process as user myuser

Do not add any new lines to the Dockerfile, just edit existing ones. The file is located at `/opt/course/16/image/Dockerfile`.

Tag your version as `v2` and make the Deployment use your updated image tag `v2`.


<details>
  <summary>Solution 16</summary>

From the default shell:

```bash
# edits the Dockerfile
cat > Dockerfile <<EOF
FROM alpine:3.12
RUN apk update && apk add vim nginx>=1.18.0
RUN addgroup -S myuser && adduser -S myuser -G myuser
COPY ./run.sh run.sh
RUN ["chmod", "+x", "./run.sh"]
USER myuser
ENTRYPOINT ["/bin/sh", "./run.sh"]
EOF

# creates, tests and pushes v2 image
podman build -t registry.killer.sh:5000/image-verify:v2 .
podman run registry.killer.sh:5000/image-verify:v2
podman push registry.killer.sh:5000/image-verify:v2

# edits the delployment to use v2 image
k edit deploy image-verify -n team-blue
```

</details>

## Question 17

Audit Logging has been enabled in the cluster with an Audit Policy located at `/etc/kubernetes/audit/policy.yaml` on `cluster2-controlplane1`.

Change the configuration so that only one backup of the logs is stored.

Alter the Policy in a way that it only stores logs:

* From Secret resources, level Metadata
* From "system:nodes" userGroups, level RequestResponse

After you altered the Policy make sure to empty the log file so it only contains entries according to your changes, like using `truncate -s 0 /etc/kubernetes/audit/logs/audit.log`.

NOTE: You can use jq to render json more readable. cat data.json | jq

## Question 18

Namespace `security` contains five Secrets of type Opaque which can be considered highly confidential. The latest Incident-Prevention-Investigation revealed that ServiceAccount `p.auster` had too broad access to the cluster for some time. This SA should've never had access to any Secrets in that Namespace.

Find out which Secrets in Namespace `security` this SA did access by looking at the Audit Logs under `/opt/course/18/audit.log`.

Change the password to any new string of only those Secrets that were accessed by this SA.

NOTE: You can use jq to render json more readable. cat data.json | jq

## Question 19

The Deployment `immutable-deployment` in Namespace `team-purple` should run immutable, it's created from file `/opt/course/19/immutable-deployment.yaml`. Even after a successful break-in, it shouldn't be possible for an attacker to modify the filesystem of the running container.

Modify the Deployment in a way that no processes inside the container can modify the local filesystem, only `/tmp` directory should be writeable. Don't modify the Docker image.

Save the updated YAML under `/opt/course/19/immutable-deployment-new.yaml` and update the running Deployment.

## Question 20

The cluster is running Kubernetes `1.27.6`, update it to `1.28.2`.

Use `apt` package manager and `kubeadm` for this.

## Question 21

The Vulnerability Scanner trivy is installed on your main terminal. Use it to scan the following images for known CVEs:

* `nginx:1.16.1-alpine`
* `k8s.gcr.io/kube-apiserver:v1.18.0`
* `k8s.gcr.io/kube-controller-manager:v1.18.0`
* `docker.io/weaveworks/weave-kube:2.7.0`

Identify all images that don't contain the vulnerabilities CVE-2020-10878 or CVE-2020-1967.

<details>
  <summary>Solution 21</summary>

From the default shell:

```bash
trivy image nginx:1.16.1-alpine | grep -E 'CVE-2020-10878|CVE-2020-1967'
trivy image k8s.gcr.io/kube-apiserver:v1.18.0 | grep -E 'CVE-2020-10878|CVE-2020-1967'
trivy image k8s.gcr.io/kube-controller-manager:v1.18.0 | grep -E 'CVE-2020-10878|CVE-2020-1967'
trivy image docker.io/weaveworks/weave-kube:2.7.0 | grep -E 'CVE-2020-10878|CVE-2020-1967'
```

</details>

## Question 22

YAML manifest and Dockerfile files have been shared with you in a directory.

Perform a manual static analysis and find out possible security issues with respect to unwanted credential exposure. Running processes as root is of no concern in this task.

Write the filenames which have issues into a specific file.

NOTE: In the Dockerfile and YAML manifests, assume that the referred files, folders, secrets and volume mounts are present. Disregard syntax or logic errors.

<details>
  <summary>Solution 22</summary>

From the default shell:

```bash
# this Dockerfile copies a file secret-token over, uses it and deletes it afterwards. But because of the way Docker works, every RUN, COPY and ADD command creates a new layer and every layer is persistet in the image. This means even if the file secret-token get's deleted in layer Z, it's still included with the image in layer X and Y. In this case it would be better to use for example variables passed to Docker
echo Dockerfile-mysql >> security-issues.txt

# this deployment manifest is fetching credentials from a Secret named mysecret and writes these into environment variables, but in the command of the container it's echoing these which can be directly read by any user having access to the logs
echo deployment-redis.yaml >> security-issues.txt

# this statefulset manifest expose the password directly in the environment variable definition of the container
echo statefulset-nginx.yaml >> security-issues.txt
```

</details>

## Preview Question 1

You have admin access to a Kubernetes cluster. There is also context `gianna@infra-prod` which authenticates as user `gianna` with the same cluster.

There are existing cluster-level RBAC resources in place to, among other things, ensure that user gianna can never read _Secret_ contents cluster-wide. Confirm this is correct or restrict the existing RBAC resources to ensure this.

In addition, create more RBAC resources to allow user `gianna` to create _Pods_ and _Deployments_ in _Namespaces_ `security`, `restricted` and `internal` and potentially others in the future.

<details>
  <summary>Solution Preview 1</summary>

From the default shell:

```bash
# searches for a ClusterRoleBinding:
k get clusterrolebinding -oyaml | grep gianna -A10 -B20

# views the binding
k edit clusterrolebinding gianna -o yaml

# views the clusterrole and identify a potential issue with secrets
k edit clusterrole gianna -o yaml

# checks using user impersonation
k auth can-i list secrets --as gianna
# yes
k auth can-i get secrets --as gianna
# no

# changes context
k config use-context gianna@infra-prod

# gets secrets is OK
k -n security get secrets

# get secret is failing
k -n security get secret kubeadmin-token

# gets secret contents from the list gives the actual secret value
âžœ k -n security get secrets -o yaml | grep password

# switches back to admin context
k config use-context infra-prod # back to admin context

# edits the clusterrole to remove secrets from the resources
k edit clusterrole gianna

# creates a clusterrole to manage pods and deployments
k create clusterrole gianna-additional --verb=create --resource=pods --resource=deployments

# creates the binding
k -n security create rolebinding gianna-additional --clusterrole=gianna-additional --user=gianna
k -n restricted create rolebinding gianna-additional --clusterrole=gianna-additional --user=gianna
k -n internal create rolebinding gianna-additional --clusterrole=gianna-additional --user=gianna

# checks permissions are ok
k auth can-i create pods --as gianna -n default
# no
k auth can-i create pods --as gianna -n security
# yes
k auth can-i create pods --as gianna -n restricted
# yes
k auth can-i create pods --as gianna -n internal
# yes
```

</details>

## Preview Question 2

There is an existing Open Policy Agent + Gatekeeper policy to enforce that all _Namespaces_ need to have label `security-level` set.

Extend the policy constraint and template so that all _Namespaces_ also need to set label `management-team`. Any new _Namespace_ creation without these two labels should be prevented.

Write the names of all existing _Namespaces_ which violate the updated policy in a file.

<details>
  <summary>Solution Preview 2</summary>

From the default shell:

```bash
# look at existing OPA constraints, implemeted using CRDs by Gatekeeper
k get crd
# blacklistimages.constraints.gatekeeper.sh
# configs.config.gatekeeper.sh
# constraintpodstatuses.status.gatekeeper.sh
# constrainttemplatepodstatuses.status.gatekeeper.sh
# constrainttemplates.templates.gatekeeper.sh
# requiredlabels.constraints.gatekeeper.sh
k get constraint
# blacklistimages.constraints.gatekeeper.sh/pod-trusted-images
# requiredlabels.constraints.gatekeeper.sh/namespace-mandatory-labels

# checks violations for the namespace-mandatory-label one, which we can do in the resource status, to get the namespace in violation
k describe requiredlabels namespace-mandatory-labels

# gets an overview of all namespaces
k get ns --show-labels

# reproduces the OPA error when creating a namespace without the required label
k create ns test
# Error from server ([denied by namespace-mandatory-labels] you must provide labels: {"security-level"}): error when creating "ns.yaml": admission webhook "validation.gatekeeper.sh" denied the request: [denied by namespace-mandatory-labels] you must provide labels: {"security-level"}

# edits the the constraint to add another required label
k edit requiredlabels namespace-mandatory-labels
# apiVersion: constraints.gatekeeper.sh/v1beta1
# kind: RequiredLabels
# metadata:
#   name: namespace-mandatory-labels
# spec:
#   match:
#     kinds:
#     - apiGroups:
#       - ""
#       kinds:
#       - Namespace
#   parameters:
#     labels:
#     - security-level
#     - management-team

# sees what happens (after a minute so the change is applied) and identifies another namespace in violation but not the previous one so we don't fullfill the objective
k describe requiredlabels namespace-mandatory-labels

# checks the constraint template
k get constrainttemplates

# edit the related constraint template (change count(missing) == 1 to count(missing) > 0)
kubectl edit constrainttemplates requiredlabels
# apiVersion: templates.gatekeeper.sh/v1beta1
# kind: ConstraintTemplate
# spec:
#   crd:
#     spec:
#       names:
#         kind: RequiredLabels
#       validation:
#         openAPIV3Schema:
#           properties:
#             labels:
#               items: string
#               type: array
#   targets:
#   - rego: |
#       package k8srequiredlabels
#       violation[{"msg": msg, "details": {"missing_labels": missing}}] {
#         provided := {label | input.review.object.metadata.labels[label]}
#         required := {label | label := input.parameters.labels[_]}
#         missing := required - provided
#         count(missing) > 0
#         msg := sprintf("you must provide labels: %v", [missing])
#       }
#     target: admission.k8s.gatekeeper.sh

# checks again the violation to confirm we see the two namespaces not respecting the constraint
k describe requiredlabels namespace-mandatory-labels

# writes the two namespace names in the file
echo "sidecar-injector\njeffs-playground" > namespaces.txt
```

## Preview Question 3

A security scan result shows there is an unknown miner process running on one of the _Nodes_ of our Kubernetes cluster. The report states that the process is listening on port `6666`.

Kill the process and delete the binary.

<details>
  <summary>Solution Preview 3</summary>

From the worker node:

```bash
# searches for a process listening on port 6666 (the command didn't report any on the control plane note)
netstat -plnt | grep 6666
# tcp6       0      0 :::6666                 :::*        LISTEN      9591/system-atm     

# searches in a different way
lsof -i :6666
# COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
# system-at 9591 root    3u  IPv6  47760      0t0  TCP *:6666 (LISTEN)

# checks the magic /proc directory for the full process path:
ls -lh /proc/9591/exe
# lrwxrwxrwx 1 root root 0 Sep 26 16:10 /proc/9591/exe -> /bin/system-atm

# kills the process
kill -9 9591

# delete the binary
rm /bin/system-atm
```

</details>
